# -*- coding: utf-8 -*-
"""hog_compare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q_zuCufOHm-we0eK27l4UtRAxl__-Q56
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/Research_workarea/malariadetection/SML-Malaria-Detection-master.zip

import os
import cv2
import copy
import csv
import random
import pickle
import numpy as np
import pandas as pd
import itertools
from scipy.stats import randint
from itertools import cycle
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
from sklearn import preprocessing
from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from scipy.sparse import csr_matrix
from scipy import stats
from my_ml_lib import DataManipulationTools, MetricTools, PlotTools
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
# Import different classifiers
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import PCA
from skimage.feature import hog, local_binary_pattern
import pickle
import tensorflow as tf
from pathlib import Path
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv3D, MaxPool3D, Flatten, Dense
from tensorflow.keras.layers import Input, Dropout, Activation, Dense, BatchNormalization, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
# import the necessary packagest
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import concatenate
import numpy as np
import argparse
import locale

def get_hog(images, name, save=True):
    result = np.array([hog(img, block_norm='L2') for img in images])

    if save:
        save_feature(result, name)

    return result

!unzip /content/drive/MyDrive/Research_workarea/malariadetection/cell_images.zip

def preprocess(image):
  image = tf.cast(image, tf.float32)
  #image = tf.image.resize(image, (64, 64))
  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)

  image = image[None, ...]
  return image

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
np.random.seed(1000)
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
import keras
from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout
from keras.models import Sequential
import os
import cv2
from PIL import Image
import tensorflow as tf

label=[]
dataset = []
SIZE=64
DATA_DIR = '/content/cell_images/'
parasitized_images = os.listdir(DATA_DIR + 'Parasitized/')
for i, image_name in enumerate(parasitized_images):
    if (image_name.split('.')[1] == 'png'):
            image = cv2.imread(DATA_DIR + 'Parasitized/' + image_name)
            image = Image.fromarray(image, 'RGB')
            image = image.resize((SIZE, SIZE))
            image=np.array(image)
            image=preprocess(image)
            image=np.array(image)
            dataset.append(image)
            label.append(0)

uninfected_images = os.listdir(DATA_DIR + 'Uninfected/')
for i, image_name in enumerate(uninfected_images):
    try:
        if (image_name.split('.')[1] == 'png'):
            image = cv2.imread(DATA_DIR + 'Uninfected/' + image_name)
            image = Image.fromarray(image, 'RGB')

            image = image.resize((SIZE, SIZE))
            image=np.array(image)
            image=preprocess(image)
            image=np.array(image)
            dataset.append(image)
            label.append(1)
    except Exception:
        print("Could not read image {} with name {}".format(i, image_name))

for i in range(len(dataset)):
    dataset[i] = dataset[i].reshape(64, 64, 3)

from tensorflow.keras.utils import to_categorical

train_files, test_files, train_labels, test_labels = train_test_split(dataset, to_categorical(np.array(label)), test_size=0.3, random_state=42)
train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=42)
print(len(train_files), len(val_files), len(test_files))
print(len(train_labels),len(val_labels),len(test_labels))

def save_feature(feature, name):
    # saving all our feature vectors in pickled file
    with open('cache/' + name + '.pkl', 'wb') as fp:
        pickle.dump(csr_matrix(feature), fp)

    print(f'Feature saved with name cache/{name}.pkl')

def load_feature(feature_name):
    return pickle.load(open(feature_name, 'rb')).A

print(len(train_files))
print(train_files[0].shape)

hog_train = get_hog(train_files, name='hog_train', save=True)
hog_val = get_hog(val_files, name='hog_val', save=True)
hog_test = get_hog(test_files, name='hog_test', save=True)

hog_train = load_feature('cache/hog_train.pkl')
hog_val = load_feature('cache/hog_val.pkl')
hog_test = load_feature('cache/hog_test.pkl')

train_files=np.array(train_files)
val_files=np.array(val_files)
test_files=np.array(test_files)
train_labels=np.array(train_labels)
val_labels=np.array(val_labels)
test_labels=np.array(test_labels)

# import the necessary packages
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
def create_mlp(dim, regress=False):
	# define our MLP network
	model = Sequential()

	model.add(Dense(512, input_dim=dim, activation="relu"))
	#model.add(Dense(4, activation="relu"))
	# check to see if the regression node should be added
	if regress:
		model.add(Dense(1, activation="linear"))
	# return our model
	return model

BATCH_SIZE = 64
NUM_CLASSES = 2
EPOCHS = 10
INPUT_SHAPE = (64, 64, 3)

def create_cnn(regress=False):
    inp = tf.keras.layers.Input(shape=INPUT_SHAPE)
    conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3),activation='relu', padding='same')(inp)
    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(pool1)
    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(pool2)
    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)
    flat = tf.keras.layers.Flatten()(pool3)
    hidden1 = tf.keras.layers.Dense(512, activation='relu')(flat)
    drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)
    hidden2 = tf.keras.layers.Dense(512, activation='relu')(drop1)
    drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)
    if regress:
		    x = Dense(1, activation="linear")(x)
	  # construct the CNN
    model = Model(inp, drop2)
	  # return the CNN
    return model

# create the MLP and CNN models
mlp = create_mlp(hog_train.shape[1], regress=False)
cnn = create_cnn(regress=False)
# create the input to our final set of layers as the *output* of both
# the MLP and CNN
combinedInput = concatenate([mlp.output, cnn.output])
# our final FC layer head will have two dense layers, the final one
# being our regression head
x = Dense(512, activation="relu")(combinedInput)
x = tf.keras.layers.Dropout(rate=0.3)(x)
out = tf.keras.layers.Dense(2, activation='sigmoid')(x)
# our final model will accept categorical/numerical data on the MLP
# input and images on the CNN input, outputting a single value (the
# predicted price of the house)
model = Model(inputs=[mlp.input, cnn.input], outputs=out)
model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])

# compile the model using mean absolute percentage error as our loss,
# implying that we seek to minimize the absolute percentage difference
# between our price *predictions* and the *actual prices*

# train the model
print("[INFO] training model...")
model.fit(
	x=[hog_train, train_files], y=train_labels,
	validation_data=([hog_val, val_files], val_labels),
	batch_size=BATCH_SIZE, epochs=6,verbose=1)
# make predictions on the testing data
#print("[INFO] predicting house prices...")
#preds = model.predict([testAttrX, testImagesX])\
model.save('mymodel_malariamodhog.h5')
from google.colab import files
files.download('mymodel_malariamodhog.h5')

from tensorflow.keras.models import load_model
model=load_model("/content/mymodel_malariamodhog.h5")

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_hog.png', show_shapes=True, show_layer_names=True)



print("Test_Accuracy: {:.2f}%".format(model.evaluate([hog_test, test_files], np.array(test_labels))[1]*100))

preds = model.predict([hog_test, test_files])
print(preds)

X_test_filtered=[]
y_test_original=[]
image_probs_orig=[]
hog_test_filtered=[]

count=0
for i in range(len(test_files)):

    if count >= 117:
        break
    data=np.array(test_files[i])
    data=data.reshape(1,64,64,3)
    #print(model.predict(np.array(data)))
    image=np.array(data)
    image = tf.convert_to_tensor(image)
    #print("image",np.max(image))
    loss_object = tf.keras.losses.CategoricalCrossentropy()
    image_probs = preds[i]
    if(image_probs[0]>0.3 and image_probs[1]>0.3):
        print(i,":",image_probs)
        X_test_filtered.append(np.array(test_files[i]))
        hog_test_filtered.append(hog_test[i])
        y_test_original.append(test_labels[i])
        image_probs_orig.append(image_probs)
        count=count+1
        print(count)

def create_adversarial_pattern(input_image, input_label):
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    image=np.array(input_image)
    image=image.reshape(1,64,64,3)
    hog_adv=get_hog(image, name='hog_adv', save=True)
    print("normal image hog", hog_adv)
    hog_adv.reshape(1,2916)
    prediction = model([hog_adv,input_image])
    #print("Prediction",prediction)
    #print("end prediction")
    loss = loss_object(input_label, prediction)
    #print("input label",input_label)
    #print("prediction",prediction)
    #print("loss",loss)
    print(loss.shape)

  # Get the gradients of the loss w.r.t to the input image.
  gradient = tape.gradient(loss, input_image)
  #print("gradient",gradient)
  # Get the sign of the gradients to create the perturbation
  signed_grad = tf.sign(gradient)
  return signed_grad

def display_images(image, description):
  data=np.array(image)
  data=data.reshape(1,64,64,3)
  hog_adv=get_hog(data, name='hog_adv', save=True)
  print("adversarial hog: ",hog_adv)
  hog_adv.reshape(1,2916)
  label=model.predict([hog_adv,image])
  image_probs_adv.append(label)
  plt.figure()
  plt.imshow(image[0]*0.5+0.5)
  plt.savefig('adversarial004.png')
  plt.title(label)
  plt.show()

y_test_filtered=[]
def argmax(array):
  index, value = 0, array[0]
  for i,v in enumerate(array):
    if v > value:
      index, value = i,v
  return index
X_test_adversary=[]
hog_test_filtered=np.array(hog_test_filtered)
X_test_filtered=np.array(X_test_filtered)
print(X_test_filtered.shape)
print(hog_test_filtered.shape)

preds_filtered = model.predict([hog_test_filtered,X_test_filtered])
print(preds_filtered)

image_probs_adv=[]

for i in range(len(X_test_filtered)):
    data=np.array(X_test_filtered[i])
    data=data.reshape(1,64,64,3)
    #print(model.predict(np.array(data)))
    image=np.array(data)
    image = tf.convert_to_tensor(image)
    #print("image",np.max(image))
    loss_object = tf.keras.losses.CategoricalCrossentropy()
    hog_a=get_hog(image, name='hog_adv', save=True)
    print("normal image hog1:", hog_a)
    #hog = np.array([hog(image, block_norm='L2') ])
    image_probs = preds_filtered[i]
    image_probs=np.array(image_probs).flatten()
    #print("image probs:",image_probs)
    image_index = argmax(image_probs)
    #print("image index:",image_index)
    print("Original image with prediction: ")
    plt.figure()
    plt.title(image_probs)
    plt.imshow(X_test_filtered[i]*0.5+0.5)
    plt.show()
    y_test_filtered.append(image_index)
    label = tf.one_hot(image_index, image_probs.shape[-1])
    label = tf.reshape(label, (1, image_probs.shape[-1]))
    #print("label",label)
    #print("end label")
    #image=preprocess(image)
    #image= tf.squeeze(image,1)
    #print("im:", image.shape)
    #hog=hog_test_filtered[i]
    #hog=np.array(hog)
    #hog=hog.reshape(1,2916)
    perturbations = create_adversarial_pattern(image, label)
    epsilons=[0.01]
    descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')
                for eps in epsilons]

    for i, eps in enumerate(epsilons):
      adv_x = image + eps*perturbations
      adv_x = tf.clip_by_value(adv_x, -1, 1)
      print("adversarial image with prediction: ")
      display_images( adv_x,descriptions[i])
    X_test_adversary.append(adv_x)

print(np.array(image_probs_orig).shape)
#print(np.array(image_probs_adv))
a=np.sum(image_probs_adv, axis=1)
print(a.shape)
abspred=(abs(np.subtract(np.array(image_probs_orig),np.array(a))))
np.sum(abspred,axis=0)